{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from functools import lru_cache\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from functools import lru_cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Italian language model (Lemmatization)\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "nlp.max_length = 2000000  # Increase max length for large documents\n",
    "\n",
    "# Define stopwords for Italian\n",
    "stop_words_ntlk = set(stopwords.words('italian'))\n",
    "custom_stopwords = set([\"quindi\", \"pertanto\", \"dunque\", \"deve essere\", \"ogni caso\", \"devono essere\", \"esempio\", \"infatti\", \"invece\", \"cioè\", \"tuttavia\", \"perché\", \"solo\", \"sempre\", \"così\", \"riguardo\", \"ovvero\", \"però\", \"comunque\", \"ancora\", \n",
    "                        \"peraltro\", \"stesso\", \"tanto\", \"poiché\", \"mentre\", \"essa\", \"inoltre\", \"punto\", \"quali\", \"stessa\", \"proprio\", \"esso\", \"perche\", \"cosi\", \"cioe\", \"deve\", \"essere\", \"devono\", \"poiche\", \"possono\", \"può\", \"puo\", \"poiche\", \"pero\", \"senso\",\n",
    "                        \"particolare\", \"oltre\", \"basta\", \"secondo\", \"rispetto\", \"infine\", \"soltanto\", \"detto\", \"caso\", \"meno\", \"ragione\", \"quando\", \"basta\", \"nonche\", \"volta\", \"meno\", \"ossia\", \"tale\", \"tratta\",\n",
    "                        \"gia\", \"pur\", \"tal\", \"poi\",\"puo\",\"cio\"])\n",
    "stop_words = set(stop_words_ntlk).union(custom_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove legal references (examples: art. 123, d.lgs. 50/2016)\n",
    "    text = re.sub(r'\\b(?:art\\.|d\\.lgs\\.|l\\.) \\d+(?:/\\d+)?', '', text)\n",
    "    # Remove new lines and tabs\n",
    "    text = re.sub(r'[\\n\\t]+', ' ', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove words containing '.' and '_'\n",
    "    text = ' '.join([word for word in text.split() if '.' not in word and '_' not in word])\n",
    "    # Remove hyphens and underscores\n",
    "    text = re.sub(r'[-_]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and non-ASCII characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove very long words (likely errors or concatenated text)\n",
    "    text = ' '.join([word for word in text.split() if len(word) <= 20])\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_parquet('SpecialisticMonotopic.parquet', engine='pyarrow')\n",
    "\n",
    "# Clean and lemmatize the text data\n",
    "data = data.dropna(subset=['topic', 'text'])\n",
    "data['topic'] = data['topic'].apply(clean_text)\n",
    "data['text'] = data['text'].apply(lambda x: lemmatize_text(remove_stopwords(clean_text(x))))\n",
    "\n",
    "# Remove words with less than 4 characters\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\b\\w{1,3}\\b', '', x))\n",
    "\n",
    "data = data.drop_duplicates(subset=['topic', 'text']).reset_index(drop=True)\n",
    "\n",
    "# Remove texts with less than 10 words\n",
    "data = data[data['text'].apply(lambda x: len(x.split()) >= 10)]\n",
    "\n",
    "# Save cleaned data\n",
    "data.to_csv('SpecialisticMonotopic_cleaned_Lemmatized.csv', index=False)\n",
    "\n",
    "print(\"Data cleaning, stopwords removal, and lemmatization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-cleaned data\n",
    "data = pd.read_csv('SpecialisticMonotopic_cleaned_Lemmatized.csv')\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique topics\n",
    "unique_topics = data['topic'].unique()\n",
    "\n",
    "# Print unique topics\n",
    "print(\"Unique topics:\")\n",
    "print(unique_topics)\n",
    "\n",
    "# Optionally, you can also get the number of unique topics\n",
    "num_unique_topics = len(unique_topics)\n",
    "print(f\"\\nNumber of unique topics: {num_unique_topics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics distribution\n",
    "\n",
    "# Count the number of texts for each topic\n",
    "topic_counts = data['topic'].value_counts()\n",
    "\n",
    "# Replace spaces with underscores in topics\n",
    "#data['topic'] = data['topic'].str.replace(' ', '_')\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=topic_counts.index, y=topic_counts.values)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Number of Texts')\n",
    "plt.title('Distribution of Texts Across Topics')\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['topic'])\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_italian_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, lang='ita'):\n",
    "        for lemma in syn.lemmas(lang='ita'):\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))  # Remove duplicates\n",
    "\n",
    "def get_synonym(word):\n",
    "    synonyms = get_italian_synonyms(word)\n",
    "    if synonyms:\n",
    "        synonym = random.choice(synonyms)\n",
    "        print(f\"Original word: '{word}' | Synonym: '{synonym}'\")\n",
    "        return synonym\n",
    "    print(f\"No synonym found for: '{word}'\")\n",
    "    return word\n",
    "\n",
    "def replace_frequent_words(text, frequent_words):\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Replace frequent words with synonyms\n",
    "    for word in frequent_words:\n",
    "        synonym = get_synonym(word)\n",
    "        if synonym != word:\n",
    "            text = text.replace(word, synonym)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Group texts by topic\n",
    "grouped_texts = train_data.groupby('topic')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Get the 100 most common words for each topic\n",
    "topic_frequent_words = {}\n",
    "for _, row in grouped_texts.iterrows():\n",
    "    topic = row['topic']\n",
    "    text = row['text']\n",
    "    word_counts = Counter(text.split())\n",
    "    most_frequent = [word for word, _ in word_counts.most_common(100)]\n",
    "    topic_frequent_words[topic] = most_frequent\n",
    "\n",
    "# Replace the frequent words with synonyms in the original texts\n",
    "train_data['enriched_text'] = train_data.apply(lambda row: replace_frequent_words(row['text'], topic_frequent_words[row['topic']]), axis=1)\n",
    "\n",
    "# Display a sample of original and enriched text\n",
    "print(\"\\nSample of original and enriched text:\")\n",
    "print(train_data[['text', 'enriched_text']].head())\n",
    "\n",
    "print(\"\\nEnrichment process completed.\")\n",
    "train_data.to_csv('Enriched_Data_Topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "# Download NLTK data files (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming 'cleaned_data' is the DataFrame with the cleaned, lemmatized, and synonym replaced text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply the tokenization function to the 'enriched_text' column\n",
    "train_data['enirched_text'] = train_data['enriched_text'].apply(tokenize_text)\n",
    "\n",
    "# Apply the tokenization function to the 'enriched_text' column\n",
    "test_data['text'] = test_data['text'].apply(tokenize_text)\n",
    "\n",
    "# Display a sample of the tokenized text\n",
    "print(\"\\nSample of tokenized text:\")\n",
    "print(train_data['enriched_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 'train_data' is the DataFrame with enriched texts\n",
    "texts_train = train_data['enriched_text'].values\n",
    "labels_train = train_data['topic'].values\n",
    "\n",
    "# Calculate class weights\n",
    "unique_labels = np.unique(labels_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(unique_labels))}\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
    "categorical_labels_train = to_categorical(encoded_labels_train)\n",
    "\n",
    "# Convert texts to TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(texts_train)\n",
    "\n",
    "# Apply TruncatedSVD to reduce dimensions\n",
    "n_components = 300  # Adjust the number of components as needed\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=seed)\n",
    "X_train_reduced = svd.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Reshape the data to fit LSTM input requirements\n",
    "X_train_lstm = X_train_reduced.reshape((X_train_reduced.shape[0], 1, X_train_reduced.shape[1]))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_lstm, categorical_labels_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_val, y_val), class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to truncate labels to a maximum of 3 words\n",
    "def truncate_label(label, max_words=3):\n",
    "    return ' '.join(label.split()[:max_words])\n",
    "\n",
    "# Truncate the labels\n",
    "truncated_labels = [truncate_label(label) for label in label_encoder.classes_]\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n",
    "\n",
    "# Normalize confusion matrix by row (true labels)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_normalized, annot=False, fmt='.2f', cmap='Blues', xticklabels=truncated_labels, yticklabels=truncated_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_val_true_classes, y_val_pred_classes, target_names=truncated_labels)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['topic'].values\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Print misclassifications with percentages\n",
    "misclassifications = []\n",
    "print(\"\\nMisclassifications:\")\n",
    "for i, actual_class in enumerate(labels):\n",
    "    for j, predicted_class in enumerate(labels):\n",
    "        if i != j and i < conf_matrix.shape[0] and j < conf_matrix.shape[1] and conf_matrix[i, j] > 0:\n",
    "            count = conf_matrix[i, j]\n",
    "            percentage = conf_matrix_normalized[i, j]\n",
    "            misclassifications.append((percentage, actual_class, predicted_class, count))\n",
    "            print(f\"Actual: {actual_class}, Predicted: {predicted_class}, Count: {count}, Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# Sort and print the top 5 highest percentages of misclassifications\n",
    "misclassifications.sort(reverse=True, key=lambda x: x[0])\n",
    "top_5_misclassifications = misclassifications[:5]\n",
    "\n",
    "print(\"\\nTop 5 Highest Misclassification Percentages:\")\n",
    "for percentage, actual_class, predicted_class, count in top_5_misclassifications:\n",
    "    print(f\"Actual class '{actual_class}' misclassified as '{predicted_class}': {count} times ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('SpecialisticMonotopic_cleaned_Lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the topics you provided earlier\n",
    "\n",
    "# Direct replacement of topics with macro topics\n",
    "data['topic'] = data['topic'].replace({\n",
    "    'assicurazione contro i danni': 'Assicurazione',\n",
    "    'assicurazione obbligatoria rca': 'Assicurazione',\n",
    "    'contratto di assicurazione': 'Assicurazione',\n",
    "    'class action azione di classe': 'Procedimenti Legali',\n",
    "    'danno da irragionevole durata del processo o equa riparazione': 'Procedimenti Legali',\n",
    "    'lite temeraria': 'Procedimenti Legali',\n",
    "    'danni punitivi': 'Danni Specifici',\n",
    "    'danno biologico': 'Danni Specifici',\n",
    "    'danno erariale': 'Danni Specifici',\n",
    "    'danno esistenziale': 'Danni Specifici',\n",
    "    'danno da immissioni': 'Danni Specifici',\n",
    "    'danno morale': 'Danni Specifici',\n",
    "    'danno da morte dei congiunti': 'Danni Specifici',\n",
    "    'danno patrimoniale': 'Danni Specifici',\n",
    "    'danno da perdita di chance': 'Danni Specifici',\n",
    "    'danno alla persona': 'Danni Specifici',\n",
    "    'danno da reato': 'Danni Specifici',\n",
    "    'danno da svalutazione monetaria': 'Danni Specifici',\n",
    "    'danno da vacanza rovinata': 'Danni Specifici',\n",
    "    'danno ai veicoli': 'Danni Specifici',\n",
    "    'danno non patrimoniale': 'Danni Specifici',\n",
    "    'ingiusta detenzione': 'Violazioni Diritti Personali',\n",
    "    'passeggero terzo trasportato terzi trasportati': 'Violazioni Diritti Personali',\n",
    "    'responsabilita contrattuale': 'Responsabilità',\n",
    "    'responsabilita civile dei magistrati': 'Responsabilità',\n",
    "    'responsabilita dei genitori': 'Responsabilità',\n",
    "    'responsabilita extracontrattuale': 'Responsabilità',\n",
    "    'responsabilita dei padroni e committenti': 'Responsabilità',\n",
    "    'responsabilita per danni da circolazione stradale': 'Responsabilità',\n",
    "    'responsabilita professionale': 'Responsabilità',\n",
    "    'responsabilita medica': 'Responsabilità',\n",
    "    'responsabilita della pa': 'Responsabilità',\n",
    "    'responsabilita del produttore danno da prodotto difettoso': 'Responsabilità',\n",
    "    'responsabilita per danno da cose in custodia': 'Responsabilità'\n",
    "})\n",
    "\n",
    "print(\"Topic replacement completed.\")\n",
    "print(data)\n",
    "\n",
    "# Distribution of macro topics\n",
    "print(\"\\nDistribution of macro topics:\")\n",
    "print(data['topic'].value_counts())\n",
    "\n",
    "# Check for any unchanged topics\n",
    "print(\"\\nUnchanged topics:\")\n",
    "print(data[~data['topic'].isin(['Assicurazione', 'Procedimenti Legali', 'Danni Specifici', 'Violazioni Diritti Personali', 'Responsabilità'])]['topic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['topic'])\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_italian_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, lang='ita'):\n",
    "        for lemma in syn.lemmas(lang='ita'):\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))  # Remove duplicates\n",
    "\n",
    "def get_synonym(word):\n",
    "    synonyms = get_italian_synonyms(word)\n",
    "    if synonyms:\n",
    "        synonym = random.choice(synonyms)\n",
    "        print(f\"Original word: '{word}' | Synonym: '{synonym}'\")\n",
    "        return synonym\n",
    "    print(f\"No synonym found for: '{word}'\")\n",
    "    return word\n",
    "\n",
    "def replace_frequent_words(text, frequent_words):\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Replace frequent words with synonyms\n",
    "    for word in frequent_words:\n",
    "        synonym = get_synonym(word)\n",
    "        if synonym != word:\n",
    "            text = text.replace(word, synonym)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Group texts by topic\n",
    "grouped_texts = train_data.groupby('topic')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Get the 100 most common words for each topic\n",
    "topic_frequent_words = {}\n",
    "for _, row in grouped_texts.iterrows():\n",
    "    topic = row['topic']\n",
    "    text = row['text']\n",
    "    word_counts = Counter(text.split())\n",
    "    most_frequent = [word for word, _ in word_counts.most_common(100)]\n",
    "    topic_frequent_words[topic] = most_frequent\n",
    "\n",
    "# Replace the frequent words with synonyms in the original texts\n",
    "train_data['enriched_text'] = train_data.apply(lambda row: replace_frequent_words(row['text'], topic_frequent_words[row['topic']]), axis=1)\n",
    "\n",
    "# Display a sample of original and enriched text\n",
    "print(\"\\nSample of original and enriched text:\")\n",
    "print(train_data[['text', 'enriched_text']].head())\n",
    "\n",
    "print(\"\\nEnrichment process completed.\")\n",
    "train_data.to_csv('Enriched_Data_Macro_Topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "# Download NLTK data files (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming 'cleaned_data' is the DataFrame with the cleaned, lemmatized, and synonym replaced text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply the tokenization function to the 'enriched_text' column\n",
    "train_data['enirched_text'] = train_data['enriched_text'].apply(tokenize_text)\n",
    "\n",
    "# Apply the tokenization function to the 'enriched_text' column\n",
    "test_data['text'] = test_data['text'].apply(tokenize_text)\n",
    "\n",
    "# Display a sample of the tokenized text\n",
    "print(\"\\nSample of tokenized text:\")\n",
    "print(train_data['enriched_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 'train_data' is the DataFrame with enriched texts\n",
    "texts_train = train_data['enriched_text'].values\n",
    "labels_train = train_data['topic'].values\n",
    "\n",
    "# Calculate class weights\n",
    "unique_labels = np.unique(labels_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(unique_labels))}\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
    "categorical_labels_train = to_categorical(encoded_labels_train)\n",
    "\n",
    "# Convert texts to TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(texts_train)\n",
    "\n",
    "# Apply TruncatedSVD to reduce dimensions\n",
    "n_components = 300  # Adjust the number of components as needed\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=seed)\n",
    "X_train_reduced = svd.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Reshape the data to fit LSTM input requirements\n",
    "X_train_lstm = X_train_reduced.reshape((X_train_reduced.shape[0], 1, X_train_reduced.shape[1]))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_lstm, categorical_labels_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_val, y_val), class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to truncate labels to a maximum of 3 words\n",
    "def truncate_label(label, max_words=3):\n",
    "    return ' '.join(label.split()[:max_words])\n",
    "\n",
    "# Truncate the labels\n",
    "truncated_labels = [truncate_label(label) for label in label_encoder.classes_]\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n",
    "\n",
    "# Normalize confusion matrix by row (true labels)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_normalized, annot=False, fmt='.2f', cmap='Blues', xticklabels=truncated_labels, yticklabels=truncated_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_val_true_classes, y_val_pred_classes, target_names=truncated_labels)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['topic'].values\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Print misclassifications with percentages\n",
    "misclassifications = []\n",
    "print(\"\\nMisclassifications:\")\n",
    "for i, actual_class in enumerate(labels):\n",
    "    for j, predicted_class in enumerate(labels):\n",
    "        if i != j and i < conf_matrix.shape[0] and j < conf_matrix.shape[1] and conf_matrix[i, j] > 0:\n",
    "            count = conf_matrix[i, j]\n",
    "            percentage = conf_matrix_normalized[i, j]\n",
    "            misclassifications.append((percentage, actual_class, predicted_class, count))\n",
    "            print(f\"Actual: {actual_class}, Predicted: {predicted_class}, Count: {count}, Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# Sort and print the top 5 highest percentages of misclassifications\n",
    "misclassifications.sort(reverse=True, key=lambda x: x[0])\n",
    "top_3_misclassifications = misclassifications[:3]\n",
    "\n",
    "print(\"\\nTop 3 Highest Misclassification Percentages:\")\n",
    "for percentage, actual_class, predicted_class, count in top_3_misclassifications:\n",
    "    print(f\"Actual class '{actual_class}' misclassified as '{predicted_class}': {count} times ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
